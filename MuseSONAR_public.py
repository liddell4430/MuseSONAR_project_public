# MuseSONAR_public.py

import config
from pydantic_models import AnalysisResultModel, MetricModel, SimilarResultModel, LlmVerificationModel
import requests
from sentence_transformers import SentenceTransformer, util
import os
import numpy as np
import google.generativeai as genai
import time
from dotenv import load_dotenv
import xml.etree.ElementTree as ET 
import re
import diskcache
from concurrent.futures import ThreadPoolExecutor
import logging
import sys
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type, before_sleep_log
import traceback
# --- íƒ€ì… íŒíŠ¸ìš© ì„í¬íŠ¸ ---
from typing import List, Dict, Tuple, Optional, Any, Literal, Union # í•„ìš”í•œ íƒ€ì… ì„í¬íŠ¸
from google.generativeai.generative_models import GenerativeModel # Gemini ëª¨ë¸ íƒ€ì…
from konlpy.tag import Okt as OktType # KoNLPy Okt íƒ€ì… (Optional ì²˜ë¦¬ ìœ„í•´ ë³„ë„ ì„í¬íŠ¸)

# --- ë¡œê¹… ì„¤ì • ---
log_format = '%(asctime)s - %(levelname)s - [%(funcName)s:%(lineno)d] - %(message)s'
logging.basicConfig(level=logging.INFO,
                    format=log_format,
                    handlers=[
                        logging.FileHandler("musessonar_analysis.log", encoding='utf-8'),
                        logging.StreamHandler()
                    ])
logging.info("==================== MuseSonar ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘ ====================")
logging.info("ë¡œê¹… ì„¤ì • ì™„ë£Œ.")


# --- ìºì‹œ ì„¤ì • ---
CACHE_DIR = os.path.join(os.path.dirname(__file__), "cache_dir")
cache: Optional[diskcache.Cache] = None
try:
    cache = diskcache.Cache(CACHE_DIR)
    logging.info(f"DiskCache ì´ˆê¸°í™” ì™„ë£Œ. ìºì‹œ ë””ë ‰í† ë¦¬: {CACHE_DIR}")

    # --- ì„ì‹œ ì½”ë“œ(ë””ë²„ê¹…ìš©): ìºì‹œ í´ë¦¬ì–´ ---
    # if cache:
        # logging.warning("!!! ê°œë°œìš© ì„ì‹œ ì½”ë“œ: DiskCache ì „ì²´ ì´ˆê¸°í™” ìˆ˜í–‰ !!!")
        # cache.clear()
        # logging.warning("!!! ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ !!!")
    
except Exception as e:
    logging.critical(f"DiskCache ì´ˆê¸°í™” ì‹¤íŒ¨! ìºì‹œ ê¸°ëŠ¥ ë¹„í™œì„±í™”. ì˜¤ë¥˜: {e}")
    cache = None

# --- KoNLPy ì„í¬íŠ¸ ---
okt: Optional[OktType] = None # Okt ê°ì²´ íƒ€ì… íŒíŠ¸ (Optional)
try:
    from konlpy.tag import Okt
    okt = Okt()
    logging.info("KoNLPy Okt í˜•íƒœì†Œ ë¶„ì„ê¸° ë¡œë”© ì™„ë£Œ.")
except ImportError:
    logging.warning("konlpy ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'pip install konlpy JPype1'ìœ¼ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”. í‚¤ì›Œë“œ ì¶”ì¶œ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
    okt = None
except Exception as e:
    logging.warning(f"KoNLPy Okt ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. í‚¤ì›Œë“œ ì¶”ì¶œ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.", exc_info=True)
    okt = None


# --- í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ---
load_dotenv()

# --- í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ ì¤‘ì•™ ê²€ì¦ ---
logging.info("í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ ê²€ì¦ ì‹œì‘...")
required_env_vars: Dict[str, str] = { # ë”•ì…”ë„ˆë¦¬ íƒ€ì… íŒíŠ¸
    'GOOGLE_SEARCH_API_KEY': 'Google Custom Search API í‚¤',
    'SEARCH_ENGINE_ID': 'Google Custom Search Engine ID',
    'GOOGLE_API_KEY_GEMINI': 'Google Gemini API í‚¤'
}
missing_vars: List[str] = [] # ë¦¬ìŠ¤íŠ¸ íƒ€ì… íŒíŠ¸
env_vars: Dict[str, str] = {} # ë”•ì…”ë„ˆë¦¬ íƒ€ì… íŒíŠ¸

for var_name, description in required_env_vars.items():
    value: Optional[str] = os.getenv(var_name) # Optional[str] íƒ€ì… íŒíŠ¸
    if not value:
        missing_vars.append(f"'{var_name}' ({description})")
    else:
        env_vars[var_name] = value

if missing_vars:
    error_message: str = "ì¹˜ëª…ì  ì˜¤ë¥˜: í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ìŒ ë³€ìˆ˜ë¥¼ .env íŒŒì¼ì— ì„¤ì •í•´ì£¼ì„¸ìš”:\n - " + "\n - ".join(missing_vars)
    logging.critical(error_message)
    sys.exit(1)
else:
    GOOGLE_SEARCH_API_KEY: str = env_vars['GOOGLE_SEARCH_API_KEY'] # str íƒ€ì… íŒíŠ¸
    SEARCH_ENGINE_ID: str = env_vars['SEARCH_ENGINE_ID'] # str íƒ€ì… íŒíŠ¸
    GOOGLE_API_KEY_GEMINI: str = env_vars['GOOGLE_API_KEY_GEMINI'] # str íƒ€ì… íŒíŠ¸
    logging.info("ëª¨ë“  í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ ê²€ì¦ í†µê³¼.")

KIPRIS_API_KEY: Optional[str] = os.getenv('KIPRIS_API_KEY') # Optional[str] íƒ€ì… íŒíŠ¸
if not KIPRIS_API_KEY:
    logging.warning("KIPRIS API í‚¤(KIPRIS_API_KEY)ê°€ í™˜ê²½ ë³€ìˆ˜ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. íŠ¹í—ˆ ê²€ìƒ‰ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
else:
    logging.info("KIPRIS API í‚¤ í™•ì¸ ì™„ë£Œ.")


# ê²€ìƒ‰ ê²°ê³¼ ë° ë”•ì…”ë„ˆë¦¬ íƒ€ì… ì •ì˜ (ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸)
GoogleResultType = List[Dict[str, str]]
KiprisResultType = List[Dict[str, str]]
LlmVerificationResultType = Tuple[str, str] # LLM ê²€ì¦ ê²°ê³¼ íƒ€ì… (ìƒíƒœ ë¬¸ìì—´, ì´ìœ  ë¬¸ìì—´)
ResultDictType = Dict[str, Any] # í•„ìš”ì‹œ ë” ìƒì„¸í•˜ê²Œ ì •ì˜ ê°€ëŠ¥
SortedResultItemType = Dict[str, Any] # 'text', 'score', 'link', 'source' ë“± í¬í•¨
LlmVerificationResultsMapType = Dict[str, LlmVerificationResultType] # text -> (status, reason)
llm_verification_results: LlmVerificationResultsMapType = {}


# --- í•¨ìˆ˜ ì •ì˜ ---

# í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜
def extract_keywords(text: str, max_kw: int = config.MAX_KIPRIS_KEYWORDS) -> str:
    """KoNLPy Oktì™€ ì •ê·œì‹ì„ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ í…ìŠ¤íŠ¸ì—ì„œ ëª…ì‚¬ ë° ì˜ë¬¸/ìˆ«ì í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    logging.info(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘ (max_kw={max_kw}): '{text[:50]}...'")

    if okt is None:
        logging.warning("KoNLPy Okt ê°ì²´ê°€ ì—†ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ ë¶ˆê°€. ì›ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return text.strip()

    keywords: List[str] = []
    try:
        pos_tagged: List[Tuple[str, str]] = okt.pos(text, norm=True, stem=True)
        nouns: List[str] = [n for n, t in pos_tagged if t == 'Noun']
        logging.debug(f"í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼ (ëª…ì‚¬): {nouns}")

        alphas_digits: List[str] = re.findall(r'\b[A-Za-z]{1,5}\d*\b|\b\d+[A-Za-z]+\b', text)
        logging.debug(f"ì •ê·œì‹ ì¶”ì¶œ ê²°ê³¼ (ì˜ë¬¸/ìˆ«ì): {alphas_digits}")

        filtered_keywords: List[str] = [w for w in nouns if len(w) > 1] + alphas_digits
        logging.debug(f"í•„í„°ë§ëœ í‚¤ì›Œë“œ: {filtered_keywords}")

        seen: set[str] = set()
        unique_keywords: List[str] = []
        for k in filtered_keywords:
            if k not in seen:
                unique_keywords.append(k)
                seen.add(k)
        logging.debug(f"ì¤‘ë³µ ì œê±°ëœ í‚¤ì›Œë“œ: {unique_keywords}")

        final_keywords: str = ' '.join(unique_keywords[:max_kw]) if unique_keywords else text.strip()

        logging.info(f"ì¶”ì¶œëœ ìµœì¢… KIPRIS í‚¤ì›Œë“œ: '{final_keywords}'")
        return final_keywords

    except Exception as e:
        logging.error(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ. ì›ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.", exc_info=True)
        return text.strip()


# =============== KIPRIS íŠ¹í—ˆ ê²€ìƒ‰ í•¨ìˆ˜  ===============
# requests + tenacity + fallback + XML íŒŒì‹± ì¡°í•© ì‚¬ìš©

# KIPRIS íŠ¹í—ˆ ê²€ìƒ‰ í•¨ìˆ˜
@cache.memoize(expire=864000) if cache else lambda f: f # ìºì‹œ ë¹„í™œì„±í™” ì‹œ ë°ì½”ë ˆì´í„° ì ìš© ì•ˆ í•¨
def search_kipris_patents(query: str) -> KiprisResultType:
    """
    KIPRIS íŠ¹í—ˆ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤ (ìºì‹± ì ìš©).
    API í‚¤ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì½ì–´ì˜µë‹ˆë‹¤.
    """
    logging.info(f"KIPRIS ê²€ìƒ‰ ì‹œë„ (ìºì‹œ í™•ì¸): query='{query}'")

    api_key: Optional[str] = os.getenv('KIPRIS_API_KEY')
    if not api_key:
        logging.warning("KIPRIS API í‚¤ê°€ ì—†ì–´ íŠ¹í—ˆ ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤ (ìºì‹œ ë˜í¼ í•¨ìˆ˜).")
        return []

    try:
        logging.debug(f"ìºì‹œ ë¯¸ìŠ¤ ë˜ëŠ” ë§Œë£Œ. KIPRIS ë‚´ë¶€ ê²€ìƒ‰ í•¨ìˆ˜ í˜¸ì¶œ: query='{query}'")
        results: KiprisResultType = _search_kipris_patents_internal(query, api_key)
        logging.info(f"KIPRIS ê²€ìƒ‰ ì™„ë£Œ (ìºì‹œ ì €ì¥ë¨): query='{query}', ê²°ê³¼ {len(results)}ê°œ")
    except Exception as e:
        logging.error(f"KIPRIS ê²€ìƒ‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ (ìºì‹œ ë˜í¼): query='{query}'", exc_info=True)
        results = []

    return results

# KIPRIS API ì‘ë‹µ íƒ€ì… ì •ì˜ (XML ìš”ì†Œ ë¦¬ìŠ¤íŠ¸, None, ë˜ëŠ” False)
KiprisApiResponseType = List[ET.Element] | None | Literal[False] # Python 3.10+ Union Syntax

# ì¬ì‹œë„ ì „ ë¡œê·¸ë¥¼ ë‚¨ê¸°ê¸° ìœ„í•œ ì„¤ì • (logging ëª¨ë“ˆê³¼ ì—°ë™)
# tenacityê°€ ì¬ì‹œë„í•˜ê¸° ì „ì— INFO ë ˆë²¨ë¡œ ë¡œê·¸ë¥¼ ë‚¨ê¹€
tenacity_logger = logging.getLogger(__name__) # í˜„ì¬ ëª¨ë“ˆì˜ ë¡œê±° ì‚¬ìš©
before_sleep = before_sleep_log(tenacity_logger, logging.INFO)

# ì¬ì‹œë„ ë°ì½”ë ˆì´í„° ì„¤ì •
@retry(
    stop=stop_after_attempt(3),  # ìµœëŒ€ 3ë²ˆ ì‹œë„ (ìµœì´ˆ 1ë²ˆ + ì¬ì‹œë„ 2ë²ˆ)
    wait=wait_exponential(multiplier=1, min=2, max=10), # 2ì´ˆ, 4ì´ˆ ê°„ê²©ìœ¼ë¡œ ì¬ì‹œë„ (ìµœëŒ€ 10ì´ˆ)
    retry=retry_if_exception_type((requests.exceptions.Timeout, requests.exceptions.ConnectionError)), # Timeout ë˜ëŠ” ConnectionError ë°œìƒ ì‹œ ì¬ì‹œë„
    before_sleep=before_sleep # ì¬ì‹œë„ ì „ì— ìœ„ì—ì„œ ì„¤ì •í•œ ë¡œê·¸ ë‚¨ê¸°ê¸°
)

# KIPRIS íŠ¹í—ˆ ê²€ìƒ‰ ìš”ì²­ í•¨ìˆ˜(ì¬ì‹œë„ ë¡œì§ ì¶”ê°€ë¨)
def request_kipris(url: str, params: Dict[str, Any], search_type: str) -> KiprisApiResponseType:
    """KIPRIS API ìš”ì²­ ë° ê¸°ë³¸ ì²˜ë¦¬ (tenacity ì¬ì‹œë„ ì ìš©)"""
    logging.debug(f"KIPRIS API ìš”ì²­ ì‹œë„: Type='{search_type}', URL='{url}'")
    try:
        logging.debug(f"  ìš”ì²­ Params (ì¼ë¶€): word='{params.get('word', '')}', rows='{params.get('numOfRows')}', query(title/astrt)='{params.get('inventionTitle', 'N/A')}'")
        response = requests.get(url, params=params, headers={'User-Agent': 'MuseSonar-prototype/1.0'}, timeout=30) # timeoutì€ ê·¸ëŒ€ë¡œ ìœ ì§€
        response.raise_for_status() # HTTP 5xx ê°™ì€ ì˜¤ë¥˜ ì‹œ ì—¬ê¸°ì„œ ì˜ˆì™¸ ë°œìƒ (ê¸°ë³¸ì ìœ¼ë¡œ tenacity ì¬ì‹œë„ ì•ˆ í•¨)

        # --- ì„±ê³µì ì¸ ì‘ë‹µ ìˆ˜ì‹  í›„ ì²˜ë¦¬ ---
        logging.debug(f"KIPRIS ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ (Status: {response.status_code}). XML íŒŒì‹± ì‹œì‘...")
        xml_root: ET.Element = ET.fromstring(response.content)
        result_code: Optional[str] = xml_root.findtext('.//resultCode')
        result_msg: Optional[str] = xml_root.findtext('.//resultMsg')

        if result_code == '00':
            items: List[ET.Element] = xml_root.findall('.//item')
            if items:
                logging.info(f"KIPRIS {search_type} ê²€ìƒ‰ ì„±ê³µ: {len(items)}ê°œ ê²°ê³¼ í™•ë³´.")
                return items # ì„±ê³µ ì‹œ ê²°ê³¼ ë°˜í™˜
            else:
                logging.warning(f"KIPRIS {search_type} ê²€ìƒ‰ ì„±ê³µí–ˆìœ¼ë‚˜ ê²°ê³¼ 0ê±´.")
                return None # ì„±ê³µí–ˆìœ¼ë‚˜ ê²°ê³¼ ì—†ìŒ
        else:
            # KIPRIS API ìì²´ ì˜¤ë¥˜ (e.g., ì˜ëª»ëœ ìš”ì²­, í‚¤ ì˜¤ë¥˜ ë“±)ëŠ” ì¬ì‹œë„ ëŒ€ìƒ ì•„ë‹˜
            logging.error(f"KIPRIS {search_type} API ìì²´ ì˜¤ë¥˜ (ì¬ì‹œë„ ëŒ€ìƒ ì•„ë‹˜): {result_msg} (ì½”ë“œ: {result_code})")
            return False # API ì˜¤ë¥˜ ì‹œ False ë°˜í™˜

    # --- ì˜ˆì™¸ ì²˜ë¦¬: tenacityê°€ ì¬ì‹œë„í•  ì˜ˆì™¸ ---
    except requests.exceptions.Timeout as e:
        logging.warning(f"KIPRIS {search_type} API ìš”ì²­ ì‹œê°„ ì´ˆê³¼ (timeout=30s). Tenacity ì¬ì‹œë„ ì˜ˆì •...")
        raise e # ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œ tenacityê°€ ì¡ê³  ì¬ì‹œë„í•˜ë„ë¡ í•¨
    except requests.exceptions.ConnectionError as e:
        logging.warning(f"KIPRIS {search_type} API ì—°ê²° ì˜¤ë¥˜ ë°œìƒ. Tenacity ì¬ì‹œë„ ì˜ˆì •...")
        raise e # ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œ tenacityê°€ ì¡ê³  ì¬ì‹œë„í•˜ë„ë¡ í•¨

    # --- ì˜ˆì™¸ ì²˜ë¦¬: tenacityê°€ ì¬ì‹œë„í•˜ì§€ ì•Šì„ ì˜ˆì™¸ ---
    except requests.exceptions.RequestException as e: # Timeout, ConnectionError ì™¸ì˜ requests ì˜ˆì™¸
        logging.error(f"KIPRIS {search_type} API ìš”ì²­ ì¤‘ ê¸°íƒ€ ì˜¤ë¥˜ ë°œìƒ (ì¬ì‹œë„ ëŒ€ìƒ ì•„ë‹˜): {e}", exc_info=True)
        return False # ì¬ì‹œë„ ì•ˆ í•  ì˜¤ë¥˜ ì‹œ False ë°˜í™˜
    except ET.ParseError as e: # XML íŒŒì‹± ì˜¤ë¥˜
        logging.error(f"KIPRIS {search_type} API ì‘ë‹µ XML íŒŒì‹± ì˜¤ë¥˜ (ì¬ì‹œë„ ëŒ€ìƒ ì•„ë‹˜): {e}", exc_info=True)
        try:
            # íŒŒì‹± ì˜¤ë¥˜ ì‹œ ì‘ë‹µ ë‚´ìš© ì¼ë¶€ ë¡œê¹… (ë””ë²„ê¹… ë„ì›€)
            logging.debug(f"XML íŒŒì‹± ì˜¤ë¥˜ ì‹œ ì‘ë‹µ ë‚´ìš© (ì¼ë¶€): {response.text[:500]}...")
        except NameError: pass # response ê°ì²´ê°€ ì—†ì„ ìˆ˜ë„ ìˆìŒ
        return False # ì¬ì‹œë„ ì•ˆ í•  ì˜¤ë¥˜ ì‹œ False ë°˜í™˜
    except Exception as e: # ê·¸ ì™¸ ëª¨ë“  ì˜ˆìƒì¹˜ ëª»í•œ ì˜ˆì™¸
        logging.error(f"KIPRIS {search_type} ê²€ìƒ‰ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ (ì¬ì‹œë„ ëŒ€ìƒ ì•„ë‹˜)", exc_info=True)
        return False # ì¬ì‹œë„ ì•ˆ í•  ì˜¤ë¥˜ ì‹œ False ë°˜í™˜

# KIPRIS íŠ¹í—ˆ ê²€ìƒ‰ ë‚´ë¶€ í•¨ìˆ˜ 
def _search_kipris_patents_internal(query: str, api_key: str) -> KiprisResultType:
    """
    KIPRIS Open APIë¥¼ í˜¸ì¶œí•˜ì—¬ íŠ¹í—ˆ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. (ë‚´ë¶€ í•¨ìˆ˜)
    3ë‹¨ê³„ Fallback ì ìš©.
    """
    logging.info(f"KIPRIS ë‚´ë¶€ ê²€ìƒ‰ ì‹œì‘: query='{query}'")
    if not api_key:
        logging.error("KIPRIS API í‚¤ê°€ ì—†ìŒ (ë‚´ë¶€ í•¨ìˆ˜). ê²€ìƒ‰ ë¶ˆê°€.")
        return []

    patent_data: KiprisResultType = []
    search_keywords: str = extract_keywords(query)

    # --- 1ë‹¨ê³„: í‚¤ì›Œë“œ ê¸°ë°˜ Advanced Search ---
    logging.info(f"KIPRIS 1ë‹¨ê³„ ì‹œë„: Advanced Search (í‚¤ì›Œë“œ='{search_keywords}', max_rows={config.KIPRIS_ADVANCED_SEARCH_ROWS})")
    url_advanced: str = "https://plus.kipris.or.kr/kipo-api/kipi/patUtiModInfoSearchSevice/getAdvancedSearch"
    params_advanced: Dict[str, Any] = {
        'word': '', 'inventionTitle': search_keywords, 'astrtCont': search_keywords,
        'patent': 'true', 'utility': 'true', 'numOfRows': config.KIPRIS_ADVANCED_SEARCH_ROWS,
        'pageNo': 1, 'sortSpec': 'OPD', 'descSort': 'true', 'ServiceKey': api_key
    }
    items: KiprisApiResponseType = request_kipris(url_advanced, params_advanced, "Advanced(Keyword)")

    # --- 2ë‹¨ê³„: ì›ë¬¸ ê¸°ë°˜ Word Search ---
    fallback_reason: str = ""
    if items is None or items is False:
        fallback_reason = "API ì˜¤ë¥˜" if items is False else "ê²°ê³¼ ì—†ìŒ"
        logging.info(f"KIPRIS 1ë‹¨ê³„ ê²°ê³¼({fallback_reason}). 2ë‹¨ê³„ ì‹œë„: Word Search (ì›ë¬¸='{query[:50]}...', max_rows={config.KIPRIS_WORD_SEARCH_ROWS})")
        url_word_orig: str = "https://plus.kipris.or.kr/kipo-api/kipi/patUtiModInfoSearchSevice/getWordSearch"
        params_word_orig: Dict[str, Any] = { 'word': query, 'year': 0, 'patent': 'true', 'utility': 'true', 'numOfRows': config.KIPRIS_WORD_SEARCH_ROWS, 'pageNo': 1, 'ServiceKey': api_key }
        items = request_kipris(url_word_orig, params_word_orig, "Word(Original)")

    # --- 3ë‹¨ê³„: í‚¤ì›Œë“œ ê¸°ë°˜ Word Search ---
    if items is None or items is False:
        fallback_reason = "API ì˜¤ë¥˜" if items is False else "ê²°ê³¼ ì—†ìŒ"
        if search_keywords != query.strip():
            logging.info(f"KIPRIS 2ë‹¨ê³„ ê²°ê³¼({fallback_reason}). 3ë‹¨ê³„ ì‹œë„: Word Search (í‚¤ì›Œë“œ='{search_keywords}', max_rows={config.KIPRIS_WORD_SEARCH_ROWS})")
            url_word_kw: str = "https://plus.kipris.or.kr/kipo-api/kipi/patUtiModInfoSearchSevice/getWordSearch"
            params_word_kw: Dict[str, Any] = { 'word': search_keywords, 'year': 0, 'patent': 'true', 'utility': 'true', 'numOfRows': config.KIPRIS_WORD_SEARCH_ROWS, 'pageNo': 1, 'ServiceKey': api_key }
            items = request_kipris(url_word_kw, params_word_kw, "Word(Keyword)")
        else:
            logging.info("KIPRIS 3ë‹¨ê³„ ê²€ìƒ‰ ê±´ë„ˆëœ€ (ì¶”ì¶œëœ í‚¤ì›Œë“œê°€ ì›ë¬¸ê³¼ ë™ì¼).")

    # --- ìµœì¢… ê²°ê³¼ ì²˜ë¦¬ ---
    if isinstance(items, list) and items: # itemsê°€ listì´ê³  ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°
        logging.info(f"KIPRIS ìµœì¢… ê²€ìƒ‰ ì„±ê³µ. íŒŒì‹± ì‹œì‘ (items: {len(items)}ê°œ)")
        patent_data = parse_kipris_items(items)
    else:
        logging.warning(f"KIPRIS ìµœì¢… íŠ¹í—ˆ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: query='{query}'")
        patent_data = []

    logging.info(f"KIPRIS ë‚´ë¶€ ê²€ìƒ‰ ì™„ë£Œ: ìµœì¢… ê²°ê³¼ {len(patent_data)}ê°œ")
    return patent_data

# KIPRIS íŠ¹í—ˆ XML íŒŒì‹±
def parse_kipris_items(items: List[ET.Element]) -> KiprisResultType:
    """KIPRIS API ì‘ë‹µì˜ <item> ë¦¬ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    logging.debug(f"KIPRIS XML ì•„ì´í…œ íŒŒì‹± ì‹œì‘ (ì…ë ¥ item ìˆ˜: {len(items)})")
    parsed_data: KiprisResultType = []
    skipped_count: int = 0
    for i, item in enumerate(items):
        try:
            title: str = item.findtext('inventionTitle', '').strip()
            abstract: str = item.findtext('astrtCont', '').strip()
            app_num: str = item.findtext('applicationNumber', '').strip()
            link: str = f"https://kpat.kipris.or.kr/kpat/searchLogina.do?next=MainSearch&target=pat_reg&Method=biblioTM&INPUT_TYPE=applno&query={app_num}" if app_num else ""
            clean_abstract: str = abstract if abstract and abstract != "ë‚´ìš© ì—†ìŒ." else ""

            if title or clean_abstract:
                content: str = f"{title}: {clean_abstract}" if title and clean_abstract else title
                parsed_data.append({
                        'text': content, 'title': title, 'abstract': clean_abstract,
                        'link': link, 'source': 'KIPRIS Patent'
                    })
            else:
                skipped_count += 1
                logging.debug(f"Item {i+1} ê±´ë„ˆëœ€: ì œëª©ê³¼ ìœ íš¨í•œ ì´ˆë¡ ëª¨ë‘ ì—†ìŒ (app_num: {app_num})")

        except Exception as e:
            logging.error(f"KIPRIS ì•„ì´í…œ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ (Item index: {i}): {e}", exc_info=True)
            skipped_count += 1

    logging.debug(f"KIPRIS XML ì•„ì´í…œ íŒŒì‹± ì™„ë£Œ (ê²°ê³¼: {len(parsed_data)}ê°œ, ê±´ë„ˆëœ€: {skipped_count}ê°œ)")
    return parsed_data

# ============= KIPRIS íŠ¹í—ˆ ê²€ìƒ‰í•¨ìˆ˜ ì¢…ë£Œ ============= 


# êµ¬ê¸€ ê²€ìƒ‰í•¨ìˆ˜
@cache.memoize(expire=864000) if cache else lambda f: f
def google_search(query: str, num_results: int = 10) -> GoogleResultType:
    """
    Google ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤ (ìºì‹± ì ìš©).
    API í‚¤ì™€ CX IDëŠ” ë‚´ë¶€ì ìœ¼ë¡œ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì½ì–´ì˜µë‹ˆë‹¤.
    """
    logging.info(f"Google ê²€ìƒ‰ ì‹œë„ (ìºì‹œ í™•ì¸): query='{query}', num={num_results}")

    api_key: Optional[str] = os.getenv('GOOGLE_SEARCH_API_KEY')
    cx: Optional[str] = os.getenv('SEARCH_ENGINE_ID')
    if not api_key or not cx:
        logging.error("Google Search API í‚¤/ID í™˜ê²½ ë³€ìˆ˜ ëˆ„ë½ (ìºì‹œ ë˜í¼). ê²€ìƒ‰ ë¶ˆê°€.")
        return []

    try:
        logging.debug(f"ìºì‹œ ë¯¸ìŠ¤ ë˜ëŠ” ë§Œë£Œ. Google ë‚´ë¶€ ê²€ìƒ‰ í•¨ìˆ˜ í˜¸ì¶œ: query='{query}', num={num_results}")
        results: GoogleResultType = _google_search_internal(query, api_key, cx, num_results)
        logging.info(f"Google ê²€ìƒ‰ ì™„ë£Œ (ìºì‹œ ì €ì¥ë¨): query='{query}', ê²°ê³¼ {len(results)}ê°œ")
    except Exception as e:
        logging.error(f"Google ê²€ìƒ‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ (ìºì‹œ ë˜í¼): query='{query}'", exc_info=True)
        results = []

    return results

# êµ¬ê¸€ ê²€ìƒ‰ ë‚´ë¶€ í•¨ìˆ˜
def _google_search_internal(query: str, api_key: str, cx: str, num_results: int = 10) -> GoogleResultType:
    """Google Custom Search APIë¥¼ í˜¸ì¶œí•˜ì—¬ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°˜í™˜"""
    logging.info(f"Google ë‚´ë¶€ ê²€ìƒ‰ ì‹œì‘: query='{query}', num={num_results}")
    search_url: str = "https://www.googleapis.com/customsearch/v1"
    headers: Dict[str, str] = {'User-Agent': 'MuseSONAR - prototype/1.0'}
    params: Dict[str, Any] = {'key': api_key, 'cx': cx, 'q': query, 'num': num_results}

    try:
        logging.debug(f"Google API ìš”ì²­ ì‹œì‘: URL='{search_url}'")
        logging.debug(f"  ìš”ì²­ Params (ì¼ë¶€): q='{query}', num='{num_results}'")

        response = requests.get(search_url, params=params, headers=headers, timeout=20)
        response.raise_for_status()
        logging.debug(f"Google API ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ (Status: {response.status_code}). JSON íŒŒì‹± ì‹œì‘...")
        search_results_json: Dict[str, Any] = response.json()
        result_data: GoogleResultType = []

        if 'items' in search_results_json:
            logging.debug(f"Google API ì‘ë‹µì—ì„œ '{len(search_results_json['items'])}'ê°œì˜ ì•„ì´í…œ ë°œê²¬.")
            for item in search_results_json['items']:
                title: str = item.get('title', '')
                snippet: str = item.get('snippet', '')
                link: str = item.get('link', '')
                if title or snippet:
                    content: str = f"{title}: {snippet}" if title and snippet else title if title else snippet
                    result_data.append({'text': content, 'link': link, 'source': 'Google Search'})
            logging.info(f"Google ê²€ìƒ‰ íŒŒì‹± ì™„ë£Œ: {len(result_data)}ê°œì˜ ìœ íš¨ ê²°ê³¼ í™•ë³´.")
            return result_data
        else:
            logging.warning("Google ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤ ('items' í‚¤ ì—†ìŒ).")
            return []
    except requests.exceptions.Timeout:
        logging.error("Google Search API ìš”ì²­ ì‹œê°„ ì´ˆê³¼ (timeout=20s).")
        return []
    except requests.exceptions.RequestException as e:
        logging.error(f"Google Search API ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        return []
    except Exception as e:
        logging.error(f"Google ê²€ìƒ‰ ì²˜ë¦¬ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ", exc_info=True)
        return []

# êµ¬ê¸€ ê²€ìƒ‰ ë°œì·Œ í•¨ìˆ˜
def build_excerpt(full_text: str) -> str:
    """ì…ë ¥ í…ìŠ¤íŠ¸ì—ì„œ LLM ê²€ì¦ì— ì‚¬ìš©í•  ìŠ¤ë‹ˆí«ì„ ìƒì„±í•©ë‹ˆë‹¤. íŠ¹ì • í‚¤ì›Œë“œ í¬í•¨ ì‹œ ê¸¸ì´ë¥¼ ëŠ˜ë¦½ë‹ˆë‹¤."""
    # í‚¤ì›Œë“œê°€ í…ìŠ¤íŠ¸ ì•ë¶€ë¶„(ì˜ˆ: 1200ì)ì— ìˆëŠ”ì§€ í™•ì¸
    # full_textê°€ 1200ìë³´ë‹¤ ì§§ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìŠ¬ë¼ì´ì‹± ì£¼ì˜
    check_range = min(len(full_text), 1200)
    if any(k in full_text[:check_range] for k in config.KW_HINTS):
        # íŒíŠ¸ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ë” ê¸¸ê²Œ ìë¦„ (ì˜ˆ: 1000ì)
        # full_textê°€ 1000ìë³´ë‹¤ ì§§ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìŠ¬ë¼ì´ì‹± ì£¼ì˜
        target_length = min(len(full_text), 1000)
        logging.debug(f"íŒíŠ¸ í‚¤ì›Œë“œ ë°œê²¬! ìŠ¤ë‹ˆí« ê¸¸ì´ë¥¼ {target_length}ìë¡œ í™•ì¥.")
        return full_text[:target_length]
    else:
        # íŒíŠ¸ í‚¤ì›Œë“œ ì—†ìœ¼ë©´ ê¸°ë³¸ ê¸¸ì´ë¡œ ìë¦„
        target_length = min(len(full_text), config.MAX_EXCERPT)
        # logging.debug(f"íŒíŠ¸ í‚¤ì›Œë“œ ì—†ìŒ. ìŠ¤ë‹ˆí« ê¸°ë³¸ ê¸¸ì´ {target_length}ì ì ìš©.") # ë¡œê·¸ê°€ ë„ˆë¬´ ë§ì„ ìˆ˜ ìˆì–´ ì£¼ì„ ì²˜ë¦¬
        return full_text[:target_length]


# LLM 2ì°¨ ê²€ì¦ í•¨ìˆ˜
@cache.memoize(expire=86400) if cache else lambda f: f # 1ì¼ ìºì‹±
def verify_similarity_with_llm_cached(user_idea: str,
                                    search_text_excerpt: str,
                                    source_type_mapped: str,
                                    model_llm: Optional[GenerativeModel],
                                    prompt_ver: str = config.PROMPT_VERSION) -> LlmVerificationResultType:
    """[ìºì‹œ ë˜í¼] LLM ê²€ì¦ ê²°ê³¼ë¥¼ ìºì‹œì—ì„œ ì°¾ê±°ë‚˜ ë‚´ë¶€ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤. (í”„ë¡¬í”„íŠ¸ ë²„ì „ ìºì‹œ í‚¤ í¬í•¨)"""
    # í•´ë‹¹llm 2ì°¨ê²€ì¦ ë¡œì§ì€ ë¹„ê³µê°œ ì²˜ë¦¬ ì˜ì—­ì…ë‹ˆë‹¤

# LLM 2ì°¨ ê²€ì¦ ë‚´ë¶€ í•¨ìˆ˜
def _verify_similarity_with_llm_internal(user_idea: str, search_text_excerpt: str, source_type_mapped: str, model_llm: GenerativeModel) -> LlmVerificationResultType:
    """[ë‚´ë¶€ í•¨ìˆ˜] LLM APIë¥¼ í˜¸ì¶œí•˜ì—¬ ìœ ì‚¬ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤."""
    logging.debug(f"LLM ë‚´ë¶€ ê²€ì¦ ì‹œì‘: source='{source_type_mapped}'")
    # model_llmì€ í˜¸ì¶œ ì „ì— None ì²´í¬ë¨ (ì—¬ê¸°ì„œëŠ” GenerativeModel íƒ€ì…ìœ¼ë¡œ ê°€ì •)

    # í•´ë‹¹llm 2ì°¨ê²€ì¦ ë¡œì§ì€ ë¹„ê³µê°œ ì²˜ë¦¬ ì˜ì—­ì…ë‹ˆë‹¤

# LLM 2ì°¨ ê²€ì¦ ë‚´ë¶€ í•¨ìˆ˜
def verify_similarity_with_llm(user_idea: str, hit: Dict[str, Any], model_llm: Optional[GenerativeModel]) -> LlmVerificationResultType:
    """
    LLM ê²€ì¦ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤ (ìºì‹± ì ìš©).
    ì…ë ¥ ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ê³  ìºì‹œëœ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
    """
    # í•´ë‹¹llm 2ì°¨ê²€ì¦ ë¡œì§ì€ ë¹„ê³µê°œ ì²˜ë¦¬ ì˜ì—­ì…ë‹ˆë‹¤

# ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜
def calculate_MuseSONAR_score(final_rating: str, jhgan_avg_score: float, llm_yes_ratio: float, num_to_verify: int) -> int:
    """ìµœì¢… ë“±ê¸‰ê³¼ ì„¸ë¶€ ì§€í‘œë¥¼ ë°”íƒ•ìœ¼ë¡œ ê³ ìœ ì„± ì ìˆ˜(0-100) ê³„ì‚°"""
    logging.debug(f"MuseSonar ì ìˆ˜ ê³„ì‚° ì‹œì‘: rating='{final_rating}', avg_score={jhgan_avg_score:.2f}, yes_ratio={llm_yes_ratio:.2f}, num_verify={num_to_verify}")
    base_score: int = 0
    adjustment: int = 0 # ë°˜ì˜¬ë¦¼ í›„ ì •ìˆ˜ê°€ ë˜ë¯€ë¡œ int

    # í•´ë‹¹ ì ìˆ˜ ê³„ì‚° ë¡œì§ì€ ë¹„ê³µê°œ ì²˜ë¦¬ ì˜ì—­ì…ë‹ˆë‹¤.

# ë“±ê¸‰ ê²°ì • í•¨ìˆ˜
def determine_originality(average_score: float, verified_similar_count: int, llm_yes_ratio: float, has_search_results_flag: bool, max_similarity_score: float) -> Tuple[str, str, str]:
    """
    í‰ê·  ìœ ì‚¬ë„, LLM ê²€ì¦ ê²°ê³¼, ìµœê³  ìœ ì‚¬ë„ ë“±ì„ ì¢…í•©í•˜ì—¬
    ì„¸ë¶„í™”ëœ ê³ ìœ ì„± ë“±ê¸‰ ë° í•´ì„ ë©”ì‹œì§€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
    """
    logging.debug(f"ê³ ìœ ì„± ë“±ê¸‰ ê²°ì • ì‹œì‘: avg_score={average_score:.2f}, yes_cnt={verified_similar_count}, yes_ratio={llm_yes_ratio:.2f}, has_results={has_search_results_flag}, max_sim={max_similarity_score:.2f}")

    final_rating: str = "í‰ê°€ ë¶ˆê°€"
    interpretation_message: str = "ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê³ ìœ ì„± ë“±ê¸‰ì„ í‰ê°€í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤."
    conditional_warning: str = ""

    # --- í•´ë‹¹ ë“±ê¸‰ ê²°ì • ë¡œì§ì€ ë¹„ê³µê°œ ì²˜ë¦¬ ì˜ì—­ì…ë‹ˆë‹¤ ---
    


# ìµœì¢… í‰ê°€ í•¨ìˆ˜
def display_results_dashboard(results: AnalysisResultModel) -> None: # ì¸ì íƒ€ì…ì„ AnalysisResultModelë¡œ ë³€ê²½
    """êµ¬ì¡°í™”ëœ AnalysisResultModel ê°ì²´ë¥¼ ëŒ€ì‹œë³´ë“œ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥"""
    logging.info("\n" + "="*30 + " ì•„ì´ë””ì–´ ê³ ìœ ì„± ìµœì¢… í‰ê°€ " + "="*30)

    # ê°ì²´ ì†ì„±ìœ¼ë¡œ ì ‘ê·¼ (e.g., results.get('rating') -> results.rating)
    rating: Optional[str] = results.rating # Optionalì¼ ìˆ˜ ìˆìŒ
    score: Optional[int] = results.score
    interpretation: Optional[str] = results.interpretation # Optionalì¼ ìˆ˜ ìˆìŒ

    logging.info(f"\nğŸ“Š ì•„ì´ë””ì–´ ê³ ìœ ì„± í‰ê°€: {rating if rating else 'N/A'}") 
    if score is not None:
        logging.info(f"â­ ê³ ìœ ì„± ì ìˆ˜: {score} / 100")
    else:
        logging.info(f"â­ ê³ ìœ ì„± ì ìˆ˜: ê³„ì‚° ë¶ˆê°€ ë˜ëŠ” ì •ë³´ ë¶€ì¡±")
    if rating: 
      logging.info("-" * (len(f"ğŸ“Š ì•„ì´ë””ì–´ ê³ ìœ ì„± í‰ê°€: {rating}") + 2))
    if interpretation: # interpretationì´ ìˆì„ ë•Œë§Œ ì¶œë ¥
        for line in interpretation.splitlines():
            logging.info(line)

    warning: Optional[str] = results.warning
    if warning:
        for line in warning.splitlines():
            logging.warning(line)

    logging.info("\nğŸ” í‰ê°€ ê·¼ê±° ìƒì„¸:")
    metrics: Optional[MetricModel] = results.metrics # Optionalì¼ ìˆ˜ ìˆìŒ
    if not metrics or not metrics.combined_results_found: # metrics ê°ì²´ ì¡´ì¬ ë° í”Œë˜ê·¸ í™•ì¸
        logging.info("  - ì›¹/íŠ¹í—ˆ ê²€ìƒ‰ ê²°ê³¼: ì•„ì´ë””ì–´ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ë¶„ì„ ë¶ˆê°€.")
    else:
        density: Optional[float] = metrics.concept_density_percentage
        relevant_count: int = metrics.relevant_search_results_count
        if density is not None:
            logging.info(f"  - ì›¹/íŠ¹í—ˆ ê°œë… ë°€ë„ (í‰ê·  ìœ ì‚¬ë„): {density:.2f}% (ê´€ë ¨ì„± ë†’ì€ {relevant_count}ê°œ ê²°ê³¼ ê¸°ì¤€)")
        else:
            logging.info("  - ì›¹/íŠ¹í—ˆ ê°œë… ë°€ë„: ê´€ë ¨ì„± ë†’ì€ ì •ë³´ ì—†ìŒ")

        evidence_rate: Optional[float] = metrics.evidence_discovery_rate_percentage
        attempts: int = metrics.verification_attempts
        evidence_count: int = metrics.evidence_count
        verify_threshold: float = metrics.verification_threshold_percentage
        if attempts > 0 and evidence_rate is not None: # evidence_rate None ì²´í¬ ì¶”ê°€
            logging.info(f"  - êµ¬ì²´ì  êµ¬í˜„ ì¦ê±° (ë°œê²¬ìœ¨): {evidence_rate:.1f}% ({evidence_count}ê°œ / {attempts}ê°œ ê²€ì¦)")
        elif relevant_count > 0:
            logging.info(f"  - êµ¬ì²´ì  êµ¬í˜„ ì¦ê±°: ê²€ì¦ ëŒ€ìƒ ê²°ê³¼ ì—†ìŒ (ìœ ì‚¬ë„ < {verify_threshold:.0f}%)")
        else:
            logging.info(f"  - êµ¬ì²´ì  êµ¬í˜„ ì¦ê±°: í•´ë‹¹ ì‚¬í•­ ì—†ìŒ")

    logging.info("\n" + "="*20 + " ì°¸ê³ : ê°€ì¥ ìœ ì‚¬í•œ ì›¹/íŠ¹í—ˆ ì •ë³´ (ìƒìœ„ 5ê°œ) " + "="*20)
    top_results: List[SimilarResultModel] = results.top_similar_results # ë¦¬ìŠ¤íŠ¸ ì§ì ‘ ì ‘ê·¼
    if top_results:
        for item in top_results: # itemì€ ì´ì œ SimilarResultModel ê°ì²´
            rank: int = item.rank
            sim_pct: float = item.similarity_percentage
            source: str = item.source
            content: str = item.content_preview
            logging.info(f"\n#{rank}. ìœ ì‚¬ë„: {sim_pct:.2f}% (ì¶œì²˜: {source})")
            logging.info(f"   - ë‚´ìš©: {content}")
            # link: Optional[str] = item.link # í•„ìš”ì‹œ ì‚¬ìš©
            # if link: logging.info(f"   - ë§í¬: {link}")

            llm_verification: Optional[LlmVerificationModel] = item.llm_verification
            if llm_verification: # LlmVerificationModel ê°ì²´ ì¡´ì¬ í™•ì¸
                status: Optional[str] = llm_verification.status
                reason: Optional[str] = llm_verification.reason
                status_icon: str = {"Yes": "âœ…", "No": "âŒ", "Unclear": "â“", "Error": "âš ï¸", "Skipped": "â­ï¸"}.get(status, "") if status else "" # status None ì²´í¬
                logging.info(f"   - LLM ê²€ì¦ (êµ¬ì²´ì  êµ¬í˜„ ì¦ê±°): {status_icon} {status if status else 'Unknown'}")
                if reason and status not in ["Skipped"]:
                    for line in reason.splitlines():
                        logging.info(f"     ã„´ ì´ìœ /ì˜¤ë¥˜: {line}")
            else:
                logging.info(f"   - LLM ê²€ì¦: ìˆ˜í–‰ë˜ì§€ ì•ŠìŒ")
    else:
        logging.info("í‘œì‹œí•  ìœ ì‚¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

logging.debug(f"create_results_dictionary í˜¸ì¶œë¨. llm_verification_results ë‚´ìš© (ì¼ë¶€): {list(llm_verification_results.items())[:2]}") 

#ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ìƒì„± í•¨ìˆ˜
def create_results_dictionary(
    final_rating: str,
    MuseSONAR_score: Optional[int],
    interpretation_message: str,
    conditional_warning: str,
    average_score: float,
    num_filtered_results: int,
    llm_yes_ratio: float,
    verified_similar_count: int,
    num_to_verify: int,
    sorted_results: List[SortedResultItemType], # SortedResultItemTypeì€ Dict[str, Any] í˜•íƒœì˜€ìŒ
    llm_verification_results: LlmVerificationResultsMapType,
    has_combined_results_flag: bool,
    high_similarity_threshold: float
) -> AnalysisResultModel: # ë°˜í™˜ íƒ€ì…ì„ AnalysisResultModelë¡œ ë³€ê²½
    """
    ëª¨ë“  í‰ê°€ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  êµ¬ì¡°í™”ëœ Pydantic ëª¨ë¸(AnalysisResultModel) ê°ì²´ë¡œ ìƒì„±
    """
    logging.debug("ê²°ê³¼ ëª¨ë¸(AnalysisResultModel) ìƒì„± ì‹œì‘.")

    # --- ìƒìœ„ 5ê°œ ê²°ê³¼ ì¶”ì¶œ ë° ì¬ì •ë ¬ (ê¸°ì¡´ ë¡œì§ ìœ ì§€) ---
    top_5_raw: List[SortedResultItemType] = sorted_results[:5]
    logging.debug(f"ì •ë ¬ ì „ ìƒìœ„ ê²°ê³¼ ì¶”ì¶œ (ìµœëŒ€ 5ê°œ): ì›ë³¸ {len(top_5_raw)}ê°œ.")
# ê²°ê³¼ ì •ë ¬ í•¨ìˆ˜
    def sort_key_for_top5(item: SortedResultItemType) -> Tuple[int, float]:
        llm_status: Optional[str] = None
        text_key: Optional[str] = item.get('text') # Optional[str] ë¡œ ëª…ì‹œí•˜ëŠ” ê²ƒì´ ë” ì •í™•
        logging.debug(f"sort_key: item text (first 30char): '{text_key[:30] if text_key else 'N/A'}'")

        # if ë¸”ë¡ ì‹œì‘
        if text_key and llm_verification_results and text_key in llm_verification_results:
            llm_status = llm_verification_results[text_key][0]
            # if ì¡°ê±´ì´ ì°¸ì¼ ë•Œì˜ ë¡œê·¸ (LLM ìƒíƒœê°€ ìˆì„ ë•Œ)
            logging.debug(f"sort_key: LLM status FOUND for '{text_key[:30] if text_key else 'N/A'}': {llm_status}")
        # else ë¸”ë¡ ì‹œì‘ (if ì¡°ê±´ì´ ê±°ì§“ì¼ ë•Œ)
        else:
            # if ì¡°ê±´ì´ ê±°ì§“ì¼ ë•Œì˜ ë¡œê·¸ (LLM ìƒíƒœê°€ ì—†ì„ ë•Œ)
            logging.debug(f"sort_key: No LLM status for '{text_key[:30] if text_key else 'N/A'}'. Reason - text_key: {bool(text_key)}, llm_results_exist: {bool(llm_verification_results)}, text_key_in_map: {text_key in llm_verification_results if text_key and llm_verification_results else 'N/A'}")
        # if-else ë¸”ë¡ ë

        similarity_score: float = item.get('score', 0.0)
        llm_priority: int = 0 if llm_status == "Yes" else 1
        similarity_priority: float = -similarity_score # ì ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ ì‘ì€ ê°’ì´ ë˜ì–´ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ ì‹œ ì•ì— ì˜´
        logging.debug(f"sort_key: Final priorities for '{text_key[:30] if text_key else 'N/A'}': llm_p={llm_priority}, sim_p={similarity_priority:.4f} (calculated_status: {llm_status})")
        return (llm_priority, similarity_priority)

    logging.debug("ìƒìœ„ ê²°ê³¼ ì¬ì •ë ¬ ì‹œì‘ (LLM 'Yes' ìš°ì„ , ë‹¤ìŒ ìœ ì‚¬ë„ ë‚´ë¦¼ì°¨ìˆœ)...")
    top_5_sorted_for_display: List[SortedResultItemType] = sorted(top_5_raw, key=sort_key_for_top5)
    logging.debug(f"ìƒìœ„ ê²°ê³¼ ì¬ì •ë ¬ ì™„ë£Œ. ìµœì¢… í‘œì‹œë  ê²°ê³¼ ìˆ˜: {len(top_5_sorted_for_display)}ê°œ. ì •ë ¬ëœ ê²°ê³¼ (ì¼ë¶€): {[{'text': r.get('text', '')[:20], 'score': r.get('score'), 'llm_status': llm_verification_results.get(r.get('text', ''), (None,))[0]} for r in top_5_sorted_for_display]}") # ì •ë ¬ ê²°ê³¼ í™•ì¸ ë¡œê·¸ ì¶”ê°€

    # --- Pydantic ëª¨ë¸ ê°ì²´ ìƒì„± ---
    logging.debug("Pydantic ëª¨ë¸ ë°ì´í„° êµ¬ì„± ì‹œì‘...")

    # 1. MetricModel ìƒì„±
    metrics_data = MetricModel(
        concept_density_percentage=average_score if num_filtered_results > 0 else None,
        evidence_discovery_rate_percentage=llm_yes_ratio * 100 if num_to_verify > 0 else None,
        evidence_count=verified_similar_count,
        verification_attempts=num_to_verify,
        relevant_search_results_count=num_filtered_results,
        combined_results_found=has_combined_results_flag,
        verification_threshold_percentage=high_similarity_threshold * 100
    )

    # 2. top_similar_results ë¦¬ìŠ¤íŠ¸ ìƒì„± (SimilarResultModel ê°ì²´ í¬í•¨)
    top_results_models: List[SimilarResultModel] = []
    for i, r in enumerate(top_5_sorted_for_display):
        llm_ver_data: Optional[LlmVerificationModel] = None
        text_key = r.get('text')
        if text_key and llm_verification_results and text_key in llm_verification_results: 
            status, reason = llm_verification_results[text_key]
            llm_ver_data = LlmVerificationModel(status=status, reason=reason)

        similar_result = SimilarResultModel(
            rank=i + 1,
            similarity_percentage=r.get('score', 0.0) * 100,
            content_preview=r.get('text', '')[:150] + "..." if len(r.get('text', '')) > 150 else r.get('text', ''),
            link=r.get('link'), # .get() ì‚¬ìš©ìœ¼ë¡œ None ê°€ëŠ¥ì„± ì²˜ë¦¬
            source=r.get('source', 'Unknown'),
            llm_verification=llm_ver_data
        )
        top_results_models.append(similar_result)

    # 3. ìµœì¢… AnalysisResultModel ìƒì„±
    analysis_result_model = AnalysisResultModel(
        rating=final_rating,
        score=MuseSONAR_score, # Optional[int]
        interpretation=interpretation_message,
        warning=conditional_warning if conditional_warning else None, # ë¹ˆ ë¬¸ìì—´ì´ë©´ None
        metrics=metrics_data, # ìœ„ì—ì„œ ìƒì„±í•œ MetricModel ê°ì²´
        top_similar_results=top_results_models # ìœ„ì—ì„œ ìƒì„±í•œ SimilarResultModel ë¦¬ìŠ¤íŠ¸
    )

    logging.info("ê²°ê³¼ ëª¨ë¸(AnalysisResultModel) ìƒì„± ì™„ë£Œ.")
    return analysis_result_model # ë”•ì…”ë„ˆë¦¬ ëŒ€ì‹  Pydantic ëª¨ë¸ ê°ì²´ ë°˜í™˜

# ì•„ì´ë””ì–´ ë¶„ì„ í•¨ìˆ˜
def analyze_idea(user_text_original: str,
                sbert_model: SentenceTransformer,
                gemini_model: Optional[GenerativeModel]) -> AnalysisResultModel:
    """
    ì…ë ¥ëœ ì•„ì´ë””ì–´ í…ìŠ¤íŠ¸ì˜ ê³ ìœ ì„±ì„ ë¶„ì„í•˜ê³  ê²°ê³¼ë¥¼ AnalysisResultModel ê°ì²´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì˜¤ë¥˜ ë°œìƒ ì‹œ AnalysisResultModelì˜ 'error' í•„ë“œì— ë©”ì‹œì§€ë¥¼ ë‹´ì•„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    logging.info(f"===== MuseSonar ë¶„ì„ ì‹œì‘ =====")
    logging.info(f"ì…ë ¥ ì•„ì´ë””ì–´: '{user_text_original}'")
    # --- 1. ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì£¼ì… ë°©ì–´: ì…ë ¥ í…ìŠ¤íŠ¸ í•„í„°ë§ (ì¼ë¶€ ê³µê°œ) ---
    filtered_user_text = user_text_original
    # ë§¤ìš° ê¸°ë³¸ì ì¸ ìœ„í—˜ íŒ¨í„´ ì˜ˆì‹œ (ì •ê·œì‹ ì‚¬ìš©, ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
    # ì˜ˆ: "ignore ... instructions", "disregard ... prompt" ë“±
    # ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ íŒ¨í„´ í•„ìš”í•  ìˆ˜ ìˆìŒ
    potential_injection_patterns = [
        r'ignore\s+(all\s+)?(previous|prior|above|following)\s+instructions?',
        r'disregard\s+(all\s+)?(previous|prior|above|following)\s+instructions?', ...
        # ì¶”ê°€ì ì¸ ìœ„í—˜ íŒ¨í„´ë“¤ ì¶”ê°€ ì˜ˆì •
    ]

    for pattern in potential_injection_patterns:
        # re.IGNORECASE í”Œë˜ê·¸ë¡œ ëŒ€ì†Œë¬¸ì ë¬´ì‹œ
        # íŒ¨í„´ ë°œê²¬ ì‹œ, í•´ë‹¹ ë¶€ë¶„ì„ ì œê±°í•˜ê³  ê²½ê³  ë¡œê·¸ ë‚¨ê¹€
        filtered_user_text, num_subs = re.subn(pattern, '', filtered_user_text, flags=re.IGNORECASE)
        if num_subs > 0:
            logging.warning(f"ì ì¬ì  í”„ë¡¬í”„íŠ¸ ì£¼ì… íŒ¨í„´ ê°ì§€ ë° ì œê±°: '{pattern}'")
            ...(ì¼ë¶€ ê³µê°œ)

    # --- 2. í•„ìˆ˜ ì„¤ì • í™•ì¸ (ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´) ---
    if not GOOGLE_SEARCH_API_KEY or not SEARCH_ENGINE_ID: 
        # ì´ ì¡°ê±´ì€ ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘ ì‹œ ê²€ì¦ì„ í†µê³¼í–ˆë‹¤ë©´ ì´ë¡ ì ìœ¼ë¡œ ë°œìƒí•˜ì§€ ì•Šì•„ì•¼ í•¨
        # í•˜ì§€ë§Œ ì•ˆì „ ì¥ì¹˜ë¡œ ë‚¨ê²¨ë‘ 
        error_msg = "í™˜ê²½ ë³€ìˆ˜ ì˜¤ë¥˜: Google Search API í‚¤ ë˜ëŠ” Engine IDê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (.env íŒŒì¼ í™•ì¸ í•„ìš”)"
        logging.critical(error_msg)
        return AnalysisResultModel(error=error_msg)
    # SBERT ëª¨ë¸ í™•ì¸
    if not sbert_model:
        error_msg = "ëª¨ë¸ ì˜¤ë¥˜: SBERT ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        logging.critical(error_msg)
        return AnalysisResultModel(error=error_msg)
    # Gemini ëª¨ë¸ ë¡œê¹…
    if not gemini_model:
        logging.warning("LLM ê²€ì¦ ê±´ë„ˆëœ€: Gemini ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    # --- 3. ë³€ìˆ˜ ì´ˆê¸°í™” ---
    search_results_data_raw: GoogleResultType = []
    patent_data: KiprisResultType = []
    combined_data_raw: List[Dict[str, str]] = []
    combined_data: List[Dict[str, str]] = []

    # --- 4. ë™ì‹œ ê²€ìƒ‰ ì‹¤í–‰ ---
    try:
        with ThreadPoolExecutor(max_workers=2) as executor:
            logging.info("--- Google ë° KIPRIS ê²€ìƒ‰ ë™ì‹œ ìš”ì²­ ì‹œì‘ ---")

            # Google ê²€ìƒ‰ ì‘ì—… ì œì¶œ 
            future_google = executor.submit(google_search, user_text_to_analyze)
            logging.info("Google ê²€ìƒ‰ ì‘ì—… ì œì¶œë¨.")

            # KIPRIS ê²€ìƒ‰ ì‘ì—… ì œì¶œ 
            future_kipris = None
            if KIPRIS_API_KEY: 
                future_kipris = executor.submit(search_kipris_patents, user_text_to_analyze)
                logging.info("KIPRIS ê²€ìƒ‰ ì‘ì—… ì œì¶œë¨.")
            else:
                logging.warning("KIPRIS API í‚¤ê°€ ì—†ì–´ íŠ¹í—ˆ ê²€ìƒ‰ ì‘ì—…ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

            # Google ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° (
            logging.debug("Google ê²€ìƒ‰ ê²°ê³¼ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘...")
            search_results_data_raw = future_google.result() # ì—¬ê¸°ì„œ ì˜ˆì™¸ ë°œìƒ ì‹œ ì•„ë˜ except ë¸”ë¡ìœ¼ë¡œ ì´ë™
            logging.info(f"Google ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì‹  ì™„ë£Œ ({len(search_results_data_raw)}ê°œ).")

            # KIPRIS ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° 
            if future_kipris:
                logging.debug("KIPRIS ê²€ìƒ‰ ê²°ê³¼ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘...")
                patent_data = future_kipris.result() # ì—¬ê¸°ì„œ ì˜ˆì™¸ ë°œìƒ ì‹œ ì•„ë˜ except ë¸”ë¡ìœ¼ë¡œ ì´ë™
                logging.info(f"KIPRIS ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì‹  ì™„ë£Œ ({len(patent_data)}ê°œ).")

        logging.info("--- ëª¨ë“  ê²€ìƒ‰ ìš”ì²­ ì²˜ë¦¬ ì™„ë£Œ ---")

    except Exception as e:
        # ë™ì‹œ ê²€ìƒ‰ ì¤‘ ì–´ë–¤ ì´ìœ ë¡œë“  ì˜ˆì™¸ ë°œìƒ ì‹œ
        error_msg = f"ì™¸ë¶€ ë°ì´í„° ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        logging.error(error_msg, exc_info=True)
        return AnalysisResultModel(error=error_msg) # ì˜¤ë¥˜ ëª¨ë¸ ë°˜í™˜

    # --- 4. ê²°ê³¼ í†µí•© ë° ì¤‘ë³µ ì œê±° ---
    logging.debug("ì›¹/íŠ¹í—ˆ ê²€ìƒ‰ ê²°ê³¼ í†µí•© ì‹œì‘...")
    # (íƒ€ì… ì²´í¬ ë° í†µí•© ë¡œì§ì€ ì´ì „ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€)
    if isinstance(search_results_data_raw, list):
        combined_data_raw.extend(search_results_data_raw)
    # ... (patent_data í†µí•© ë¡œì§) ...
    logging.debug(f"ê²°ê³¼ í†µí•© ì™„ë£Œ (í†µí•© ì „: {len(combined_data_raw)}ê°œ).")

    seen_texts: set[str] = set()
    combined_data = []
    skipped_non_dict: int = 0
    for item in combined_data_raw:
        if isinstance(item, dict) and 'text' in item and isinstance(item['text'], str):
            text_content: str = item['text']
            if text_content not in seen_texts:
                combined_data.append(item)
                seen_texts.add(text_content)
        else:
            logging.warning(f"í†µí•© ë°ì´í„° í•­ëª© íƒ€ì… ì˜¤ë¥˜ ë˜ëŠ” 'text' í‚¤/ê°’ ì—†ìŒ: {item}")
            skipped_non_dict += 1
    logging.info(f"ì¤‘ë³µ ì œê±° í›„ ë¶„ì„ ëŒ€ìƒ ë°ì´í„°: {len(combined_data)}ê°œ (ì›ë˜ {len(combined_data_raw)}ê°œ, í˜•ì‹ ì˜¤ë¥˜ {skipped_non_dict}ê°œ ì œì™¸)")

    # --- 5. ë¶„ì„ ê°€ëŠ¥ ë°ì´í„° ì—†ìŒ ("ì •ë³´ ë¶€ì¡±") ì²˜ë¦¬ ---
    if not combined_data:
        # ê²€ìƒ‰ì€ ì‹œë„í–ˆìœ¼ë‚˜ ìœ íš¨í•œ ë¶„ì„ ëŒ€ìƒ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°
        logging.warning("ë¶„ì„ ê²°ê³¼: ê´€ë ¨ì„± ìˆëŠ” ì›¹/íŠ¹í—ˆ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ìœ íš¨í•œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        final_rating = "ì •ë³´ ë¶€ì¡±"
        interpretation_message = "ì•„ì´ë””ì–´ì™€ ê´€ë ¨ëœ ì›¹ ë˜ëŠ” íŠ¹í—ˆ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ìœ íš¨í•œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ì•„ì´ë””ì–´ë¥¼ ë‹¤ì‹œ ì…ë ¥í•˜ê±°ë‚˜ í‚¤ì›Œë“œë¥¼ í™•ì¸í•´ ë³´ì„¸ìš”."
        # MetricModel ê¸°ë³¸ê°’ ìƒì„± (í•„ìˆ˜ í•„ë“œ ìœ„ì£¼)
        metrics = MetricModel(
            evidence_count=0, verification_attempts=0, relevant_search_results_count=0,
            combined_results_found=bool(combined_data_raw), # ê²€ìƒ‰ ì‹œë„ ì—¬ë¶€
            verification_threshold_percentage=config.HIGH_SIMILARITY_THRESHOLD * 100
        )
        # ì •ë³´ ë¶€ì¡± ìƒíƒœ ëª¨ë¸ ë°˜í™˜ (ì˜¤ë¥˜ëŠ” ì•„ë‹˜)
        result_model = AnalysisResultModel(
            rating=final_rating,
            interpretation=interpretation_message,
            metrics=metrics
            # score, warning, top_similar_resultsëŠ” ê¸°ë³¸ê°’(None, None, []) ì‚¬ìš©
        )
        logging.warning("===== MuseSonar ë¶„ì„ ì™„ë£Œ (ê²°ê³¼ ì •ë³´ ë¶€ì¡±) =====")
        return result_model

    # --- 6. ìœ ì‚¬ë„ ë¶„ì„ ë° LLM ê²€ì¦ ---
    average_score: float = 0.0
    num_filtered_results: int = 0
    num_to_verify: int = 0
    verified_similar_count: int = 0
    llm_verification_results: LlmVerificationResultsMapType = {}
    all_results_with_scores: List[SortedResultItemType] = []
    max_similarity_score: float = 0.0
    sorted_results: List[SortedResultItemType] = []

    try:
        combined_texts: List[str] = [item['text'] for item in combined_data if 'text' in item]

        if not combined_texts: # ì´ ê²½ìš°ëŠ” ìœ„ combined_data ì²´í¬ì—ì„œ ê±¸ëŸ¬ì¡Œì–´ì•¼ í•˜ì§€ë§Œ ë°©ì–´ ì½”ë“œ
            raise ValueError("ë¶„ì„í•  í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤ (combined_texts ë¹„ì–´ìˆìŒ).")

        logging.info("--- SBERT ìœ ì‚¬ë„ ê³„ì‚° ë° ë¶„ì„ ì‹œì‘ ---")
        # ì„ë² ë”© ê³„ì‚°
        logging.debug("SBERT ëª¨ë¸ ì„ë² ë”© ê³„ì‚° ì‹œì‘...")
        embeddings = sbert_model.encode([user_text_to_analyze] + combined_texts, convert_to_tensor=True)
        user_vec = embeddings[0]
        result_vecs = embeddings[1:]
        logging.debug(f"ì„ë² ë”© ê³„ì‚° ì™„ë£Œ. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ì‹œì‘...")
        cos_scores: List[float] = util.cos_sim(user_vec, result_vecs)[0].cpu().tolist()
        logging.debug(f"ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ì™„ë£Œ.")

        # ëª¨ë“  ê²°ê³¼ì— ì ìˆ˜ ë“± ë§¤í•‘
        all_results_with_scores = []
        for i, score in enumerate(cos_scores):
            if i < len(combined_data) and isinstance(combined_data[i], dict):
                all_results_with_scores.append({
                    'text': combined_data[i].get('text', ''), 'score': score,
                    'link': combined_data[i].get('link', ''), 'source': combined_data[i].get('source', 'Unknown')
                })
            else:
                logging.error(f"ê²°ê³¼ ë§¤í•‘ ì˜¤ë¥˜: ì¸ë±ìŠ¤ {i} ë˜ëŠ” combined_data[{i}] í˜•ì‹ ì˜¤ë¥˜")

        # ê´€ë ¨ì„± í•„í„°ë§
        filtered_results_list = [r for r in all_results_with_scores if r.get('score', 0.0) >= config.RELEVANCE_THRESHOLD]
        num_filtered_results = len(filtered_results_list)
        logging.info(f"ìœ ì‚¬ë„ í•„í„°ë§ ì™„ë£Œ: {num_filtered_results}ê°œ ê²°ê³¼ >= {config.RELEVANCE_THRESHOLD*100:.0f}%")

        # --- 7. í•„í„°ë§ ê²°ê³¼ 0ê°œ ("ê´€ë ¨ì„± ë†’ì€ ì •ë³´ ë¶€ì¡±") ì²˜ë¦¬ ---
        if num_filtered_results == 0:
            logging.warning(f"ë¶„ì„ ê²°ê³¼: ê´€ë ¨ì„± ë†’ì€(ìœ ì‚¬ë„ >= {config.RELEVANCE_THRESHOLD*100:.0f}%) ì›¹/íŠ¹í—ˆ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ.")
            final_rating = "ì •ë³´ ë¶€ì¡±"
            interpretation_message = f"ì•„ì´ë””ì–´ì™€ ê´€ë ¨ì„±ì´ ë†’ì€(ìœ ì‚¬ë„ {config.RELEVANCE_THRESHOLD*100:.0f}% ì´ìƒ) ì›¹ ë˜ëŠ” íŠ¹í—ˆ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì•„ì´ë””ì–´ë¥¼ ë” êµ¬ì²´í™”í•˜ê±°ë‚˜ ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì‹œë„í•´ ë³´ì„¸ìš”."
            # ìœ ì‚¬ë„ ë†’ì€ ê²°ê³¼ëŠ” ì—†ì§€ë§Œ, ì „ì²´ ê²°ê³¼ëŠ” ì •ë ¬í•´ì„œ ì°¸ê³ ìš©ìœ¼ë¡œ ì œê³µ ê°€ëŠ¥
            sorted_results = sorted(all_results_with_scores, key=lambda x: x.get('score', 0.0), reverse=True)
            # MetricModel ìƒì„±
            metrics = MetricModel(
                evidence_count=0, verification_attempts=0,
                relevant_search_results_count=num_filtered_results, # 0
                combined_results_found=True, # ê²€ìƒ‰ ê²°ê³¼ ìì²´ëŠ” ìˆì—ˆìŒ
                verification_threshold_percentage=config.HIGH_SIMILARITY_THRESHOLD * 100
            )
            # ì •ë³´ ë¶€ì¡± ìƒíƒœ ëª¨ë¸ ë°˜í™˜
            result_model = AnalysisResultModel(
                rating=final_rating,
                interpretation=interpretation_message,
                metrics=metrics,
                top_similar_results=[ # ìƒìœ„ 5ê°œ ì •ë³´ëŠ” ëª¨ë¸ì— ë§ê²Œ ë³€í™˜í•˜ì—¬ ì „ë‹¬
                    SimilarResultModel(
                        rank=i+1, similarity_percentage=r.get('score', 0.0)*100,
                        content_preview=r.get('text', '')[:150] + "...", link=r.get('link'), source=r.get('source', 'Unknown')
                    ) for i, r in enumerate(sorted_results[:5]) # LLM ê²€ì¦ì€ ì—†ìœ¼ë¯€ë¡œ None
                ]
            )
            logging.warning("===== MuseSonar ë¶„ì„ ì™„ë£Œ (ê´€ë ¨ì„± ë†’ì€ ì •ë³´ ë¶€ì¡±) =====")
            return result_model

        # --- 8. ê´€ë ¨ì„± ë†’ì€ ê²°ê³¼ ê¸°ë°˜ í†µê³„ ê³„ì‚° ë° LLM ê²€ì¦(ë¹„ê³µê°œ) ---
        ...

    except Exception as e:
        # SBERT ë¶„ì„, LLM ê²€ì¦ ë“± ì´ ë¸”ë¡ ë‚´ì—ì„œ ë°œìƒí•˜ëŠ” ëª¨ë“  ì˜ˆì™¸ ì²˜ë¦¬
        error_msg = f"ì•„ì´ë””ì–´ ë¶„ì„ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        logging.error(error_msg, exc_info=True)
        return AnalysisResultModel(error=error_msg) # ì˜¤ë¥˜ ëª¨ë¸ ë°˜í™˜

    # --- 9. ìµœì¢… í‰ê°€ ë° ê²°ê³¼ ìƒì„±(ë¹„ê³µê°œ) ---
    

    try:
        #--- 10. ë“±ê¸‰ ë° í•´ì„ ê²°ì • ---
        final_rating, interpretation_message, conditional_warning = determine_originality(
            average_score, verified_similar_count, llm_yes_ratio, has_combined_results_flag, max_similarity_score
        )
        # ì ìˆ˜ ê³„ì‚°
        if final_rating not in ["í‰ê°€ ë¶ˆê°€ (ì˜¤ë¥˜)", "ì •ë³´ ë¶€ì¡±"]: # ì˜¤ë¥˜/ì •ë³´ë¶€ì¡± ì•„ë‹ ë•Œë§Œ ì ìˆ˜ ê³„ì‚°
            score_int = calculate_MuseSONAR_score(final_rating, average_score, llm_yes_ratio, num_to_verify)
            MuseSONAR_score = score_int if score_int != -1 else None
        else:
            MuseSONAR_score = None # ì ìˆ˜ ê³„ì‚° ë¶ˆê°€

    except Exception as e:
        # ìµœì¢… í‰ê°€/ì ìˆ˜ ê³„ì‚° ì¤‘ ì˜ˆì™¸ ë°œìƒ ì‹œ
        error_msg = f"ìµœì¢… í‰ê°€ ë˜ëŠ” ì ìˆ˜ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        logging.error(error_msg, exc_info=True)
        # ì˜¤ë¥˜ ëª¨ë¸ì„ ë°˜í™˜í•˜ê±°ë‚˜, rating/interpretationì„ ì˜¤ë¥˜ ìƒíƒœë¡œ ë‘ê³  ì•„ë˜ ëª¨ë¸ ìƒì„±ìœ¼ë¡œ ë„˜ì–´ê°ˆ ìˆ˜ ìˆìŒ
        # ì—¬ê¸°ì„œëŠ” ì˜¤ë¥˜ ëª¨ë¸ì„ ë°˜í™˜í•˜ëŠ” ê²ƒì´ ë” ëª…í™•í•  ìˆ˜ ìˆìŒ
        return AnalysisResultModel(error=error_msg)

    # --- 11. ìµœì¢… ê²°ê³¼ ëª¨ë¸ ìƒì„± (create_results_dictionary í˜¸ì¶œ) ---
    try:
        results_model = create_results_dictionary(
            final_rating, MuseSONAR_score, interpretation_message, conditional_warning,
            average_score, num_filtered_results, llm_yes_ratio, verified_similar_count, num_to_verify,
            sorted_results, llm_verification_results, has_combined_results_flag, config.HIGH_SIMILARITY_THRESHOLD
        )
        logging.info("===== MuseSonar ë¶„ì„ ì™„ë£Œ =====")
        return results_model # ìµœì¢… ì„±ê³µ ì‹œ ê²°ê³¼ ëª¨ë¸ ë°˜í™˜

    except Exception as e:
        # ê²°ê³¼ ëª¨ë¸ ìƒì„± ì¤‘ ì˜ˆì™¸ ë°œìƒ ì‹œ (ë“œë¬¸ ê²½ìš°)
        error_msg = f"ìµœì¢… ê²°ê³¼ ëª¨ë¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        logging.error(error_msg, exc_info=True)
        return AnalysisResultModel(error=error_msg) # ì˜¤ë¥˜ ëª¨ë¸ ë°˜í™˜

# --- ë©”ì¸ ì‹¤í–‰ ë¡œì§ (í…ŒìŠ¤íŠ¸ìš©) ---
if __name__ == "__main__":
    logging.info("--- __main__ ë¸”ë¡ ì‹¤í–‰: í…ŒìŠ¤íŠ¸ìš© ëª¨ë¸ ë¡œë”© ì‹œì‘ ---")
    sbert_model_test: Optional[SentenceTransformer] = None # íƒ€ì… íŒíŠ¸ ì¶”ê°€
    try:
        sbert_model_test = SentenceTransformer('jhgan/ko-sroberta-multitask')
        logging.info("í…ŒìŠ¤íŠ¸ìš© Sentence Transformer ëª¨ë¸ ë¡œë”© ì™„ë£Œ (jhgan/ko-sroberta-multitask).")
    except Exception as e:
        logging.critical(f"í…ŒìŠ¤íŠ¸ìš© SBERT ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨! ë¶„ì„ ë¶ˆê°€.", exc_info=True)

    gemini_model_test: Optional[GenerativeModel] = None # íƒ€ì… íŒíŠ¸ ì¶”ê°€
    if GOOGLE_API_KEY_GEMINI:
        try:
            genai.configure(api_key=GOOGLE_API_KEY_GEMINI)
            default_model = 'models/gemini-2.0-flash' 
            model_name: str = os.getenv('GEMINI_MODEL_NAME', default_model)
            logging.info(f"ì‚¬ìš©í•  Gemini ëª¨ë¸: {model_name} (í™˜ê²½ ë³€ìˆ˜ 'GEMINI_MODEL_NAME' ë˜ëŠ” ê¸°ë³¸ê°’)")
            gemini_model_test = genai.GenerativeModel(model_name) # ìˆ˜ì •ëœ model_name ì‚¬ìš©
            logging.info(f"í…ŒìŠ¤íŠ¸ìš© Gemini API ì„¤ì • ì™„ë£Œ ({model_name} ëª¨ë¸ ì‚¬ìš©).")
        except Exception as e:
            logging.warning(f"í…ŒìŠ¤íŠ¸ìš© Gemini API ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ. LLM ê²€ì¦ ê¸°ëŠ¥ ë¹„í™œì„±í™”ë¨.", exc_info=True)
            gemini_model_test = None
    else:
        logging.warning("í…ŒìŠ¤íŠ¸ìš© Gemini API í‚¤ê°€ ì—†ì–´ LLM ê²€ì¦ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")

    logging.info("--- í…ŒìŠ¤íŠ¸ìš© ëª¨ë¸ ë¡œë”© ì™„ë£Œ ---")

    test_idea: str = "ê³ ì–‘ì´ ì‚¬ë£Œ ì˜ì–‘ë¶„ì„ ì–´í”Œ." #í…ŒìŠ¤íŠ¸ìš© ì•„ì´ë””ì–´
    logging.info(f"--- MuseSonar.py ì§ì ‘ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ ì‹œì‘ (ì•„ì´ë””ì–´: '{test_idea}') ---")

    analysis_result: AnalysisResultModel # ë°˜í™˜ íƒ€ì…ì€ AnalysisResultModel

    if sbert_model_test:
        analysis_result = analyze_idea(test_idea, sbert_model_test, gemini_model_test)
    else:
        # SBERT ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ì‹œ ì˜¤ë¥˜ ëª¨ë¸ ìƒì„±
        analysis_result = AnalysisResultModel(error="í…ŒìŠ¤íŠ¸ìš© SBERT ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ë¡œ ë¶„ì„ ë¶ˆê°€")

    # --- ê²°ê³¼ ì²˜ë¦¬: analysis_result.error í•„ë“œ í™•ì¸ ---
    if analysis_result.error:
        # ì˜¤ë¥˜ê°€ ìˆëŠ” ê²½ìš°
        logging.error(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {analysis_result.error}")
    elif analysis_result.rating is None: # ì •ìƒ ì¢…ë£Œë˜ì—ˆìœ¼ë‚˜ ë¶„ì„ ì •ë³´ ë¶€ì¡± ë“± ratingì´ ì—†ëŠ” ê²½ìš°
        logging.warning("ë¶„ì„ì€ ì™„ë£Œë˜ì—ˆìœ¼ë‚˜ ìœ ì˜ë¯¸í•œ ê²°ê³¼ë¥¼ ì–»ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ì˜ˆ: ì •ë³´ ë¶€ì¡±)")
        # ì´ ê²½ìš°ì—ë„ dashboardëŠ” í˜¸ì¶œí•˜ì—¬ ì œí•œëœ ì •ë³´ë¼ë„ í‘œì‹œ ê°€ëŠ¥
        display_results_dashboard(analysis_result)
    else:
        # ì •ìƒì ìœ¼ë¡œ ë¶„ì„ ê²°ê³¼ê°€ ë‚˜ì˜¨ ê²½ìš°
        display_results_dashboard(analysis_result) # AnalysisResultModel ê°ì²´ë¥¼ ê·¸ëŒ€ë¡œ ì „ë‹¬

    logging.info("--- MuseSonar.py ì§ì ‘ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ---")
    logging.info("==================== MuseSonar ìŠ¤í¬ë¦½íŠ¸ ì¢…ë£Œ ====================")